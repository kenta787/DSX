{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "## \u5f62\u614b\u7d20\u89e3\u6790\u306e\u6e96\u5099\n\n* \u5f62\u614b\u7d20\u89e3\u6790\u5668\u306f\u4e00\u65e6janome\u3067", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "!pip install --user janome", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 1, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): janome in /gpfs/global_fs01/sym_shared/YPProdSpark/user/scb8-24e6a99445cd7d-16c15584504a/.local/lib/python3.5/site-packages\r\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## janome\u30c6\u30b9\u30c8", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "from janome.tokenizer import Tokenizer\nt = Tokenizer()\ntokens = t.tokenize(\"\u77ed\u7de8\u5c0f\u8aac\")\nfor token in tokens:\n    print(token)", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 2, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "\u77ed\u7de8\t\u540d\u8a5e,\u4e00\u822c,*,*,*,*,\u77ed\u7de8,\u30bf\u30f3\u30da\u30f3,\u30bf\u30f3\u30da\u30f3\n\u5c0f\u8aac\t\u540d\u8a5e,\u4e00\u822c,*,*,*,*,\u5c0f\u8aac,\u30b7\u30e7\u30a6\u30bb\u30c4,\u30b7\u30e7\u30fc\u30bb\u30c4\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\n\n* \u63d0\u4f9b\u3055\u308c\u3066\u308b\u65e5\u7d4cBP\u306e\u30e1\u30fc\u30eb\u30ea\u30b9\u30c8\u3092\u8a66\u3057\u306b\u5229\u7528\n* \u5229\u7528\u3059\u308b\u305f\u3081\u306b\u30ed\u30fc\u30ab\u30eb\u3067\u3061\u3087\u3063\u3068\u524d\u51e6\u7406\u52a0\u3048\u3066\u307e\u3059\u3002\n\n```bash\nnkf -g nikkei_mail_magazine.csv\nnkf -w --overwrite nikkei_mail_magazine.csv\nnkf -g nikkei_mail_magazine.csv\nsed -i.bak \"1s/^.*$/\\\"date\\\",\\\"from\\\",\\\"subject\\\",\\\"body\\\"/\" nikkei_mail_magazine.csv\n```\n\nSJIS to UTF8\u3068header\u306e\u65e5\u672c\u8a9e\u304c\u6271\u3044\u306b\u304f\u3044\u306e\u3067\u82f1\u8a9e\u306b\u5909\u63db", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "\nfrom pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function accesses a file in your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_a2229a240bdb40ca889517c3e9fffe02(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage V3 using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'a54bf6fa500643c8be55e65ffa23044e')\n    hconf.set(prefix + '.username', '6112cea574dd4a9785108a27d74be76a')\n    hconf.set(prefix + '.password', 'Ax6THHtPEit6z?4_')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_a2229a240bdb40ca889517c3e9fffe02(name)\n\nspark = SparkSession.builder.getOrCreate()\n\ndf_data_8 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://JapanBigDataHandsOn.' + name + '/nikkei_mail_magazine.csv')\ndf_data_8.take(5)\n", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 3, 
            "outputs": [
                {
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-3-acec5feced6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mdf_data_8\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.execution.datasources.csv.CSVFileFormat'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'header'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'true'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'swift://JapanBigDataHandsOn.'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/nikkei_mail_magazine.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mdf_data_8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime/4.1.1/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ], 
                    "evalue": "", 
                    "output_type": "error", 
                    "ename": "KeyboardInterrupt"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "df_data_8.printSchema()", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "\ndf_data_6 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://JapanBigDataHandsOn.' + name + '/analytics_staff.csv')\ndf_data_6.take(5)\n", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "df_data_6.printSchema()", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "\ndf_data_7 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load('swift://JapanBigDataHandsOn.' + name + '/analytics_job.csv')\ndf_data_7.take(5)\n", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "df_data_7.printSchema()", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## \u5bfe\u8c61\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\n\n\u4e00\u65e6\u4ef6\u540d(subject)\u306e\u307f\u3092\u5bfe\u8c61\u3068\u3059\u308b\u3002\n\u672c\u6587(body)\u3092\u5916\u3057\u305f\u7406\u7531\u3068\u3057\u3066\u306f\u3001\n* \u8a18\u53f7\u7cfb(http://~)\u306e\u524d\u51e6\u7406\u9762\u5012\n* DSX\u306e\u30e1\u30e2\u30ea\u306b\u4e57\u308b\u304b\u4e0d\u5b89\u30fb\u30fb\u30fb", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "##\nsubject_rdd = df_data_8.select(\"subject\").rdd.cache()\nbody_rdd = df_data_8.select(\"body\").rdd.cache()\n\nstaff_text_rdd = df_data_6.select(\"STAFF_TEXT\").rdd.cache()\njob_text_rdd = df_data_7.select(\"JOB_TEXT\").rdd.cache()\n", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 9, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "#\u30b5\u30f3\u30d7\u30eb\u8868\u793a\n#subject_rdd.take(1)\n#body_rdd.take(1)\n#job_text_rdd.take(1)\n#staff_text_rdd.take(1)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## \u5f62\u614b\u7d20\u89e3\u6790\u3068\u54c1\u8a5e\u306e\u78ba\u8a8d\n* \u65e5\u7d4cBP\u306e\u60c5\u5831(Subject)\u306e\u78ba\u8a8d", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "subject_rdd.count() ##\u5168\u4f53\u306eINPUT\u4ef6\u6570", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 15, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "2049"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##\u5f62\u614b\u7d20\u89e3\u6790\u3055\u308c\u305f\u4e2d\u9593\u30c7\u30fc\u30bf\u306e\u751f\u6210\ntokenized_subject_rdd = subject_rdd.filter(lambda x: x.subject != None)\\\n.map(lambda x: [(token.part_of_speech,(token.surface) if (token.base_form == \"*\") else token.base_form ) for token in Tokenizer().tokenize(x.subject)]).cache()", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 18, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "tokenized_subject_rdd.flatMap(lambda x: [(token[0],token[1].lower()) for token in x])\\\n.count() ##\u30c7\u30a3\u30d5\u30a9\u30eb\u30c8\u306e\u8f9e\u66f8\u3067\u5f62\u614b\u7d20\u89e3\u6790\u53ef\u80fd\u306a\u30ef\u30fc\u30c9\u6570", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 19, 
            "outputs": [
                {
                    "execution_count": 19, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "13275"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "tokenized_subject_rdd.flatMap(lambda x:[(token[0],token[1].lower()) for token in x if (token[0].split(\",\")[0].find(\"\u540d\u8a5e\") >= 0 and token[0].split(\",\")[1].find(\"\u4e00\u822c\") >= 0)])\\\n.count() ##\u5229\u7528\u53ef\u80fd\u3068\u601d\u308f\u308c\u308b\u4e00\u822c\u540d\u8a5e", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 27, 
            "outputs": [
                {
                    "execution_count": 27, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "3110"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##\u51fa\u529b\u304c\u591a\u3044\u306e\u3067\u6ce8\u610f\uff01\uff01\uff01\uff01\ntokenized_subject_rdd.flatMap(lambda x:[(token[0],token[1].lower()) for token in x if (token[0].split(\",\")[0].find(\"\u540d\u8a5e\") >= 0 and token[0].split(\",\")[1].find(\"\u4e00\u822c\") >= 0)])\\\n.collect() ##\u5229\u7528\u53ef\u80fd\u3068\u601d\u308f\u308c\u308b\u4e00\u822c\u540d\u8a5e\n", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "##\u8868\u73fe\u7cfb\u306e\u307f\u62bd\u51fa\nsubject_input = tokenized_subject_rdd.map(lambda x:[token[1].lower() for token in x if (token[0].split(\",\")[0].find(\"\u540d\u8a5e\") >= 0 and token[0].split(\",\")[1].find(\"\u4e00\u822c\") >= 0)])\n##word2vec\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\nfrom pyspark.mllib.feature import Word2Vec\nw2v = Word2Vec()\n##subject_input\u304b\u3089\u30e2\u30c7\u30eb\u751f\u6210\nsubject_model = w2v.setNumIterations(3).fit(subject_input)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 30, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "##\u30e2\u30c7\u30eb\u304b\u308910\u4ef6\u307b\u3069\u30ef\u30fc\u30c9\u306e\u8fd1\u4f3c\u3092\u3068\u308b(ex \u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc)\nfor word,cos in subject_model.findSynonyms(\"\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc\",10):\n    print(\"word: {}, cos: {}\".format(word,cos))", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 34, 
            "outputs": [
                {
                    "text": "word: \u9769\u547d, cos: 0.9580185554272082\nword: \u30b5\u30df\u30c3\u30c8, cos: 0.9566541933327893\nword: \u30e1\u30fc\u30ab\u30fc, cos: 0.9426169766540321\nword: \u6226\u7565, cos: 0.9305059926942794\nword: \u5de5\u5834, cos: 0.9274528630415462\nword: \u624b\u6cd5, cos: 0.9264431645681588\nword: \u30a8\u30f3\u30b8\u30cb\u30a2, cos: 0.9169012429213501\nword: \u672a\u6765, cos: 0.9164756517407582\nword: mail, cos: 0.9119419929851151\nword: \u8b1b\u5ea7, cos: 0.9091678869524277\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##\u3053\u306e\u30bb\u30eb\u306f\u30a8\u30e9\u30fc\u304c\u8868\u793a\u3055\u308c\u307e\u3059\uff01\uff01\uff01\uff01\n##\u63fa\u3089\u304e\u30fb\u8a8d\u8b58\u3057\u3066\u3044\u306a\u3044\u30ef\u30fc\u30c9\u306e\u30a8\u30e9\u30fc(ex \u30bb\u30ad\u30e5\u30ea\u30c6\u30a3)\nfor word,cos in subject_model.findSynonyms(\"\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\",10):\n    print(\"word: {}, cos: {}\".format(word,cos))", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## subject\u306e\u8003\u5bdf\n* 10\u4ef6\u306e\u8fd1\u4f3c\u3057\u305f\u30ef\u30fc\u30c9\u306e\u51fa\u529b\u306f\u5168\u4f53\u7684\u306b\u9ad8\u3044\u985e\u4f3c\u5ea6\u3092\u53d6\u3063\u3066\u3044\u308b\n    * \u3053\u308c\u306fInput\u304c\u5c11\u306a\u3044 or \u304b\u306a\u308a\u504f\u308a\u304c\u3042\u308b\u305f\u3081\u540c\u4e00\u306esubject\u306e\u307f\u3060\u3068\u30ef\u30fc\u30c9\u304c\u504f\u3063\u3066\u3044\u308b\u3068\u601d\u308f\u308c\u308b\n* \u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc\u3068\u3044\u3046\u30ef\u30fc\u30c9\u306b\u5bfe\u3057\u3066\u9769\u547d\u3068\u3044\u3046\u30ef\u30fc\u30c9\u306e\u91cd\u8981\u5ea6\u304c\u9ad8\u3044\u306e\u306f\u6b63\u3057\u3044\u306e\u304b\uff1f\n    * \u666e\u901a\u306b\u8003\u3048\u3066\u7121\u99c4\u306a\u30ef\u30fc\u30c9\u30fb\u30fb\u30fb\n* \u30ef\u30fc\u30c9\u306e\u63fa\u3089\u304e\uff08\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc/\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\uff09\u306e\u6271\u3044", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "* body\u306e\u78ba\u8a8d", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "##body\u306e\u4ef6\u6570\u78ba\u8a8d\nbody_rdd.count()", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 37, 
            "outputs": [
                {
                    "execution_count": 37, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "2049"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##\u5f62\u614b\u7d20\u89e3\u6790\u3055\u308c\u305f\u4e2d\u9593\u30c7\u30fc\u30bf\u306e\u751f\u6210\ntokenized_body_rdd = body_rdd.filter(lambda x: x.body != None)\\\n.map(lambda x: [(token.part_of_speech,(token.surface) if (token.base_form == \"*\") else token.base_form ) for token in Tokenizer().tokenize(x.body)]).cache()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 40, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "tokenized_body_rdd.flatMap(lambda x: [(token[0],token[1].lower()) for token in x])\\\n.count() ##\u30c7\u30a3\u30d5\u30a9\u30eb\u30c8\u306e\u8f9e\u66f8\u3067\u5f62\u614b\u7d20\u89e3\u6790\u53ef\u80fd\u306a\u30ef\u30fc\u30c9\u6570", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 41, 
            "outputs": [
                {
                    "execution_count": 41, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "997616"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "tokenized_body_rdd.flatMap(lambda x:[(token[0],token[1].lower()) for token in x if (token[0].split(\",\")[0].find(\"\u540d\u8a5e\") >= 0 and token[0].split(\",\")[1].find(\"\u4e00\u822c\") >= 0)])\\\n.count() ##\u5229\u7528\u53ef\u80fd\u3068\u601d\u308f\u308c\u308b\u4e00\u822c\u540d\u8a5e", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 42, 
            "outputs": [
                {
                    "execution_count": 42, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "194003"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##\u8868\u73fe\u7cfb\u306e\u307f\u62bd\u51fa\nbody_input = tokenized_body_rdd.map(lambda x:[token[1].lower() for token in x if (token[0].split(\",\")[0].find(\"\u540d\u8a5e\") >= 0 and token[0].split(\",\")[1].find(\"\u4e00\u822c\") >= 0)])\n##subject_input\u304b\u3089\u30e2\u30c7\u30eb\u751f\u6210\nbody_model = w2v.setNumIterations(3).fit(body_input)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 43, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "##\u30e2\u30c7\u30eb\u304b\u308910\u4ef6\u307b\u3069\u30ef\u30fc\u30c9\u306e\u8fd1\u4f3c\u3092\u3068\u308b(ex \u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc)\nfor word,cos in body_model.findSynonyms(\"\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc\",10):\n    print(\"word: {}, cos: {}\".format(word,cos))", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 44, 
            "outputs": [
                {
                    "text": "word: \u8106\u5f31, cos: 0.724601132346835\nword: \u30b3\u30f3\u30c6\u30b9\u30c8, cos: 0.6800820966066703\nword: \u30bb\u30ad\u30e5\u30ea, cos: 0.6687638073249337\nword: ecu, cos: 0.660375655214382\nword: volkswagen, cos: 0.6568711164156256\nword: \u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc\u30cf\u30c3\u30ab\u30bd\u30f3, cos: 0.6466343062083438\nword: linux, cos: 0.6368051432735068\nword: \u30c6\u30a3\u30fc, cos: 0.6326350765407089\nword: \u5b98\u516c\u5e81, cos: 0.6229293605448595\nword: \u59d4\u54e1, cos: 0.6151273748508489\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## body\u306e\u8003\u5bdf\n* subject\u3088\u308a\u306f\u30de\u30b7\u306a\u985e\u4f3c\u5ea6\u304c\u51fa\u3066\u308b\n* linux\u306e\u30ef\u30fc\u30c9\u3068\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc\u306e\u30ef\u30fc\u30c9\u306f\u3064\u306a\u304c\u3063\u3066\u3044\u3066\u554f\u984c\u306a\u3044\u3068\u601d\u308f\u308c\u308b\n* \u5b98\u516c\u5e81\u3068\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30fc\u3082\u4eca\u56de\u306e\u30b1\u30fc\u30b9\u3067\u306f\u610f\u5473\u3092\u6301\u3061\u305d\u3046", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "* \u4eba\u6750\u60c5\u5831\u304b\u3089\u306e\u62bd\u51fa", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "staff_text_rdd.count() ##\u5168\u4f53\u306eINPUT\u4ef6\u6570", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 45, 
            "outputs": [
                {
                    "execution_count": 45, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "46483"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "##\u5f62\u614b\u7d20\u89e3\u6790\u3055\u308c\u305f\u4e2d\u9593\u30c7\u30fc\u30bf\u306e\u751f\u6210\ntokenized_staff_rdd = staff_text_rdd.filter(lambda x: x.STAFF_TEXT != None)\\\n.map(lambda x: [(token.part_of_speech,(token.surface) if (token.base_form == \"*\") else token.base_form ) for token in Tokenizer().tokenize(x.STAFF_TEXT)]).cache()", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 46, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "tokenized_staff_rdd.flatMap(lambda x: [(token[0],token[1].lower()) for token in x])\\\n.count() ##\u30c7\u30a3\u30d5\u30a9\u30eb\u30c8\u306e\u8f9e\u66f8\u3067\u5f62\u614b\u7d20\u89e3\u6790\u53ef\u80fd\u306a\u30ef\u30fc\u30c9\u6570", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "tokenized_body_rdd.flatMap(lambda x:[(token[0],token[1].lower()) for token in x if (token[0].split(\",\")[0].find(\"\u540d\u8a5e\") >= 0 and token[0].split(\",\")[1].find(\"\u4e00\u822c\") >= 0)])\\\n.count() ##\u5229\u7528\u53ef\u80fd\u3068\u601d\u308f\u308c\u308b\u4e00\u822c\u540d\u8a5e", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## \u5f62\u614b\u7d20\u89e3\u6790\u3068\u5bfe\u8c61\u8a9e\u53e5\u306e\u62bd\u51fa\n\n* \u3053\u3053\u3067\u4e00\u822c\u540d\u8a5e\u306e\u307f\u62bd\u51fa\u3057\u3066\u308b\u304c\u3001\u5bfe\u8c61\u304c\u3053\u308c\u3067\u826f\u3044\u304b\u306f\u691c\u8a3c\u5fc5\u8981\u30fb\u30fb\u30fb\n* \u4e00\u822c\u540d\u8a5e\u306e\u307f\u3060\u3068\u8a9e\u53e5\u306e\u95a2\u9023\u6027\u304c\u98db\u3093\u3067\u308b\u304d\u304c\u3059\u308b\u30fb\u30fb\u30fb\n* \u540d\u8a5e\u306e\u307f\u3060\u3068\u8a18\u53f7\u7cfb\u304c\u90aa\u9b54\u306a\u306e\u3067\u524d\u51e6\u7406\u3067\u3069\u3046\u306b\u304b\u30fb\u30fb\u30fb\n* \u4eca\u56de\u306e\u3060\u3068\u30b5\u5909\u6d3b\u7528\u3068\u304b\u3082\u5165\u308c\u305f\u307b\u3046\u304c\u826f\u3044\u304b\u3082", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "subject_input = subject_rdd.filter(lambda x: x.subject != None)\\\n.map(lambda x: [(token.part_of_speech,(token.surface) if (token.base_form == \"*\") else token.base_form ) for token in Tokenizer().tokenize(x.subject)])\\\n.map(lambda x: [token[1].lower() for token in x if (token[0].find(\"\u540d\u8a5e\") >= 0 and token[0].find(\"\u4e00\u822c\") >= 0)] )\n\nbody_input = body_rdd.filter(lambda x: x.body != None)\\\n.map(lambda x: [(token.part_of_speech,(token.surface) if (token.base_form == \"*\") else token.base_form ) for token in Tokenizer().tokenize(x.body)])\\\n.map(lambda x: [token[1].lower() for token in x if (token[0].find(\"\u540d\u8a5e\") >= 0 and token[0].find(\"\u4e00\u822c\") >= 0)] )\n\nstaff_input = staff_text_rdd.filter(lambda x: x.STAFF_TEXT != None)\\\n.map(lambda x: [(token.part_of_speech,(token.surface) if (token.base_form == \"*\") else token.base_form ) for token in Tokenizer().tokenize(x.STAFF_TEXT)])\\\n.map(lambda x: [token[1].lower() for token in x if (token[0].find(\"\u540d\u8a5e\") >= 0 and token[0].find(\"\u4e00\u822c\") >= 0)] )\n\njob_input = job_text_rdd.filter(lambda x: x.JOB_TEXT != None)\\\n.map(lambda x: [(token.part_of_speech,(token.surface) if (token.base_form == \"*\") else token.base_form ) for token in Tokenizer().tokenize(x.JOB_TEXT)])\\\n.map(lambda x: [token[1].lower() for token in x if (token[0].find(\"\u540d\u8a5e\") >= 0 and token[0].find(\"\u4e00\u822c\") >= 0)] )\n\n", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 18, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "input = subject_input.union(body_input).union(job_input).union(staff_input)\n\n#job_input.take(5)\n#staff_input.take(2)", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 21, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## word2vec\u30e2\u30c7\u30eb\u69cb\u7bc9\n\n* \u3068\u308a\u3042\u3048\u305aIteration:3\u3067\u3002\u30c7\u30fc\u30bf\u91cf\u3068\u6642\u9593\u304c\u8a31\u3059\u306a\u3089\u3082\u3046\u3061\u3087\u3044Iteration\u3042\u3052\u305f\u307b\u3046\u304c\u7cbe\u5ea6\u826f\u304f\u306a\u308b\u6c17\u304c\u3057\u3066\u308b", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "from pyspark.mllib.feature import Word2Vec\nw2v = Word2Vec()\nmodel = w2v.setNumIterations(3).fit(input)", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 22, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Synonyms\n\n* \u5358\u8a9e\u304b\u3089\u5358\u7d14\u306a\u985e\u4f3c\u5ea6\u306e\u62bd\u51fa\u3002\n* \u30c7\u30fc\u30bf\u91cf\u304c(ry\n* \u3084\u306f\u308a\u524d\u51e6\u7406\u304c\u30fb\u30fb\u30fb\u30fb\n* \u30a8\u30e9\u30fc\u51fa\u308b\u5834\u5408\u306f\u305d\u306e\u5358\u8a9e\u304c\u5b58\u5728\u3057\u3066\u306a\u3044\u53ef\u80fd\u6027\u304c\u5fae\u30ec\u5b58", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#\u5358\u8a9e\u306ecos\u3092\u53d6\u308b\nfor word,cos in model.findSynonyms(\"\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\",10):\n    print(\"word: {}, cos: {}\".format(word,cos))\n", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 11, 
            "outputs": [
                {
                    "text": "word: \u30b5\u30a4\u30d0\u30fc\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3, cos: 0.717802432021995\nword: jaspar, cos: 0.7129278982548151\nword: ota, cos: 0.686505864218765\nword: \u8105\u5a01, cos: 0.6472447891425471\nword: \u30df\u30c9\u30eb, cos: 0.6055930169931124\nword: dena, cos: 0.5891736442972841\nword: os, cos: 0.5873115911159915\nword: \u30b7\u30b9\u30c6\u30e0\u30ba, cos: 0.5794159866840655\nword: \u4e3b\u67fb, cos: 0.5651777844114697\nword: \u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8, cos: 0.5633326928571022\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## wrodcluster\u306e\u751f\u6210\n\n* word2vec\u306e\u7d50\u679c\u304b\u3089word\u306ecluster\u3092\u751f\u6210\u3002\n* \u3053\u308c\u3067\u5358\u8a9e\u6bce\u306egrouping\u306f\u53ef\u80fd\u3053\u3053\u3067\u306f\u4e00\u65e610cluster\u3002\n* \u591a\u5206\u696d\u7a2e or \u5358\u8a9etree\u306eroot\u30ce\u30fc\u30c9\u6570\u3068\u304b\u3067\u9069\u5f53\u306bcluster\u5316\u3059\u308c\u3070\u3044\u3044\u3068\u601d\u3046\u306e\u3002\n* \u3053\u3053\u304b\u3089\u6709\u5411\u30b0\u30e9\u30d5\u3092\u751f\u6210\u3059\u308b\u65b9\u6cd5\u30fb\u30fb\u30fb\u30fb\u306f\u3069\u3046\u3057\u3088\u3046\u30fb\u30fb\u30fb(\uff1b\u00b4\u0414\uff40)\n* \u3053\u3053\u3082\u7cbe\u5ea6\u306f\u3054\u5bdf\u3057\u30fb\u30fb\u30fb", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "from pyspark.mllib.clustering import KMeans, KMeansModel\nw2v_vectors = model.getVectors() #dic\nl_vec = [ [float(c) for c in v] for v in w2v_vectors.values()]\ncluster_n = 10\nkmean_model = KMeans.train(sc.parallelize(l_vec),cluster_n,5)\n\nfor key,val in w2v_vectors.items():\n    print(\"key:{},cluster:{}\".format(key,kmean_model.predict([float(conv) for conv in val])))", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## \u3068\u308a\u3042\u3048\u305a\u306e\u8ab2\u984c\n* cluster\u304b\u3089root\u30ce\u30fc\u30c9\u306e\u62bd\u51fa\n* \u95a2\u9023\u6027\u306e\u4e0a\u4e0b\u95a2\u4fc2\n* \u591a\u5206\u30ef\u30fc\u30c9\u304c\u591a\u304f\u306a\u308b\u3068\u4eca\u5ea6\u306f\u901f\u5ea6\u304c\u554f\u984c\u306b\u306a\u308b\u306e\u3067\u3001\u30ef\u30fc\u30c9\u306e\u5208\u308a\u8fbc\u307f or \u6982\u5ff5\u3067\u306e\u30af\u30e9\u30b9\u5316\n\n<br/>\u3042\u305f\u308a\u304c\u8ab2\u984c\u304b\uff1f", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## \u8907\u5408\u8a9e\u306e\u751f\u6210\n\n* \u5358\u7d14\u306a\u8907\u5408\u8a9e\u306e\u751f\u6210\u306b\u3064\u3044\u3066\u8003\u5bdf\n    * \u4eba\u540d\u306e\u751f\u6210\uff08\u59d3+\u540d\uff09\n    * \u540d\u8a5e\u5f62\u5bb9\u8a5e\u8907\u5408\uff08\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79+\u4e00\u822c\uff09\n    * \u540d\u8a5e\u30b5\u5909\u8907\u5408\uff08\u4e00\u822c+\u30b5\u5909\u63a5\u7d9a+\u63a5\u5c3e\uff09", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "combi_input = body_rdd.filter(lambda x: x.body != None)\\\n.map(lambda x: [(token.part_of_speech,(token.surface) if (token.base_form == \"*\") else token.base_form ) for token in Tokenizer().tokenize(x.body)])\\\n.map(lambda x: [(token[0],token[1].lower()) for token in x if (token[0].startswith(\"\u540d\u8a5e\")\n                                                               or token[0].startswith(\"\u52d5\u8a5e\")\n                                                               or token[0].startswith(\"\u5f62\u5bb9\u8a5e\"))])\\\n.cache()\n", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 12, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "sample = combi_input.map(lambda x: [(i,word) for i,word in enumerate(x)]).take(1)", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 13, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "source": "new_words = []\nstack = \"\"\nlast = None\nfor index,val in sample[0]:\n    if(index == 0 or index == 1):\n        continue\n    last = sample[0][index-1:index]\n    l_last = sample[0][index-2:index-1]\n    (p,surf) = val\n    p1,p2,p3,p4 = p.split(\",\")\n    [(pi,(p_p,p_surf))] = last\n    pp1,pp2,pp3,pp4 = p_p.split(\",\")\n    [(ppi,(pp_p,pp_surf))] = l_last\n    ppp1,ppp2,ppp3,ppp4 = pp_p.split(\",\")\n    n_word = p_surf + surf\n    if((pp4 == \"\u59d3\" and p4 == \"\u540d\")\n       or (pp2 == \"\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79\" and p2 == \"\u4e00\u822c\")):\n        new_words.append((pi,2,n_word,p_p))\n    if(ppp2 == \"\u4e00\u822c\" and pp2 == \"\u30b5\u5909\u63a5\u7d9a\" and p2 == \"\u63a5\u5c3e\"):\n        new_words.append((ppi,3,pp_surf + n_word,pp_p))\n        \n\nprint(new_words)", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 14, 
            "outputs": [
                {
                    "text": "[(48, 2, '\u91cd\u8981\u8ab2\u984c', '\u540d\u8a5e,\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79,*,*'), (169, 2, '\u91cd\u8981\u8ab2\u984c', '\u540d\u8a5e,\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79,*,*'), (203, 2, '\u91cd\u8981\u8ab2\u984c', '\u540d\u8a5e,\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79,*,*'), (223, 2, '\u5371\u967a\u30af\u30eb\u30de', '\u540d\u8a5e,\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79,*,*'), (357, 3, '\u30b3\u30f3\u30c6\u30b9\u30c8\u958b\u50ac\u65e5', '\u540d\u8a5e,\u4e00\u822c,*,*'), (500, 3, '\u65e5\u7cfb\u5b8c\u6210\u8eca', '\u540d\u8a5e,\u4e00\u822c,*,*'), (548, 2, '\u5b89\u5168\u74b0\u5883', '\u540d\u8a5e,\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79,*,*'), (552, 3, '\u8b1b\u5e2b\u9078\u5b9a\u4e2d', '\u540d\u8a5e,\u4e00\u822c,*,*'), (606, 3, '\u30bb\u30df\u30ca\u30fc\u7d42\u4e86\u5f8c', '\u540d\u8a5e,\u4e00\u822c,*,*'), (610, 3, '\u30cd\u30c3\u30c8\u30ef\u30fc\u30ad\u30f3\u30b0\u30d1\u30fc\u30c6\u30a3\u30fc\u4ea4\u6d41\u4f1a', '\u540d\u8a5e,\u4e00\u822c,*,*'), (722, 3, '\u5b9a\u671f\u8cfc\u8aad\u8005', '\u540d\u8a5e,\u4e00\u822c,*,*'), (834, 3, '\u90e8\u54c1\u958b\u767a\u8005', '\u540d\u8a5e,\u4e00\u822c,*,*'), (851, 3, '\u4e16\u754c\u4e2d\u7814\u7a76\u8005', '\u540d\u8a5e,\u4e00\u822c,*,*'), (879, 2, '\u4e95\u4e0a\u535a\u4e4b', '\u540d\u8a5e,\u56fa\u6709\u540d\u8a5e,\u4eba\u540d,\u59d3'), (893, 2, '\u5009\u5730\u4eae', '\u540d\u8a5e,\u56fa\u6709\u540d\u8a5e,\u4eba\u540d,\u59d3'), (971, 3, '\u79d1\u5b66\u7814\u7a76\u79d1', '\u540d\u8a5e,\u4e00\u822c,*,*'), (979, 2, '\u4e95\u4e0a\u535a\u4e4b', '\u540d\u8a5e,\u56fa\u6709\u540d\u8a5e,\u4eba\u540d,\u59d3'), (985, 3, '\u79d1\u5b66\u7814\u7a76\u79d1', '\u540d\u8a5e,\u4e00\u822c,*,*'), (996, 2, '\u5009\u5730\u4eae', '\u540d\u8a5e,\u56fa\u6709\u540d\u8a5e,\u4eba\u540d,\u59d3'), (1115, 3, '\u5b9a\u671f\u8cfc\u8aad\u8005', '\u540d\u8a5e,\u4e00\u822c,*,*')]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## \u65b0\u305f\u306aword\u3067\u306e\u7f6e\u304d\u63db\u3048", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#del sample[0][48:50]\n#sample[0].insert(48,new_words[0])\nprint(sample[0][47:51])", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 17, 
            "outputs": [
                {
                    "text": "[(47, ('\u540d\u8a5e,\u30b5\u5909\u63a5\u7d9a,*,*', '\u958b\u767a')), (48, ('\u540d\u8a5e,\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79,*,*', '\u91cd\u8981')), (49, ('\u540d\u8a5e,\u4e00\u822c,*,*', '\u8ab2\u984c')), (50, ('\u540d\u8a5e,\u30b5\u5909\u63a5\u7d9a,*,*', '\u2503'))]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "del sample[0][48:50]\nprint(sample[0][47:51])", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 18, 
            "outputs": [
                {
                    "text": "[(47, ('\u540d\u8a5e,\u30b5\u5909\u63a5\u7d9a,*,*', '\u958b\u767a')), (50, ('\u540d\u8a5e,\u30b5\u5909\u63a5\u7d9a,*,*', '\u2503')), (51, ('\u540d\u8a5e,\u30b5\u5909\u63a5\u7d9a,*,*', '\u2517\u2501\u25ce')), (52, ('\u540d\u8a5e,\u56fa\u6709\u540d\u8a5e,\u7d44\u7e54,*', 'http'))]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "sample[0].insert(48,new_words[0])\nprint(sample[0][47:51])", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 19, 
            "outputs": [
                {
                    "text": "[(47, ('\u540d\u8a5e,\u30b5\u5909\u63a5\u7d9a,*,*', '\u958b\u767a')), (48, 2, '\u91cd\u8981\u8ab2\u984c', '\u540d\u8a5e,\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79,*,*'), (50, ('\u540d\u8a5e,\u30b5\u5909\u63a5\u7d9a,*,*', '\u2503')), (51, ('\u540d\u8a5e,\u30b5\u5909\u63a5\u7d9a,*,*', '\u2517\u2501\u25ce'))]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "!id", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 29, 
            "outputs": [
                {
                    "text": "uid=37476(s321-f440ad61ef9e72-ef22f67700fb) gid=100(users) groups=100(users)\r\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": []
        }
    ], 
    "nbformat_minor": 0, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "name": "python3-spark20", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.0"
        }, 
        "language_info": {
            "pygments_lexer": "ipython3", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "version": "3.5.2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}